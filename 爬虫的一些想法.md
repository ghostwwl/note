## 清理磁盘找到了2008年的总结 ^_^
--------------------------------------------------------------------------------------------

### 最近的一点想法总结
+ Author：ghostwwl
+ Time：2008.10

--------------------------------------------------------------------------------------------
```
区分导航页面和最终页面（这个思想和Google PageRank思想一致)
任老大的想法

通过inUrl outUrl  singleUrl这3个东西可以区分出来。

链入,链出,归一后的URL

inurl:有多少个URL指向自己
outurl:本页导出多少个站内URL
singleurl:归一后的URL做计数。


爬虫要解决两个最重要的问题
1. 不迷路
2. 根据页面的重要程度定期回访

URL->下载->分析outurl->对outurl归一->给别人投票->记录归一后的url的
数量[归一的表和自己的值]->提取body内的所有正文，
将<a.*?>.*?</a>全部替换成空,然后分析专业度，记录专业词的数量

每条URL至少的几个字段   url,inurl,outurl[归一后的值],keywordcout

归一的URL积累到20个的时候就做一次专业度分析，如果发现专业度集体不够，就是跑偏了，可能会迷路。

因为有3个问题要考虑

1、不迷路
2、回访的按照重要度
3、每个页面这么多outurl，先爬哪一个？

--------------------------------------------------------------------------------------------

PageRank思想 是把所有的网络 看成一个大的网络拓扑结构（我这里假设每个站点为一个网络节点)

前提是假设用户从任意一个节点进如网络 然后可能从页面随意跳到其他节点 

首先假设本节点得分为S 然后 指向 N个其他节点 那么其他节点的从他分享这里的得分是

S/N 。计算当前随机点到(1- 因子数)/节点规模 + 分享其他节点的分数的积分求和 * 因子

这个因子数我觉得应该是 用户随机打开这个页面的概率有关的一个因子 而且还描述了 从其他

任意一个节点进入本节点的概率相关

PageRank的思想很实用 但是我觉得还有局限性 因为 不可能对所有的网络节点分析 也就是说

这个网络图不是完全的 这个问题是无法避免的问题

--------------------------------------------------------------------------------------------

HITS算法思想

HITS算法的思想很简单 认为 被转载多 或者被链入多的 节点越重要 可以看出 很简单 很实用

但是同样不可避免节点样本不完全 而出现的 误差

——————————————————————————————————————————————

PageRank 和 HITS这两个算法思想都可行 

但是都存在一个无法避免的缺陷 
1. 不知道节点有多少 无法描述

真正的 现实网络 都是假设的基础上 这个是现在无法避免的 爬虫采集节点不完全 问题

但是也不可能等到所有节点完全再去计算 一个矛盾的问题

2.还有一个问题就是 对于搜索引擎来说 并不是真正的广度优先的采集 

如果真的这样 也许永无止境 这就带来一个问题 ？

也许有的节点还没有被下载 没有纳入统计范围 而有的 节点在此算法下可能被

计算了很多次  但是这个问题是可以理解的 我们不可能收集齐所有的节点

然后我们也不可能对 初期通过算法认为重要的节点 停止回访采集

3. 这就面临一个 怎么能有效降低 节点内的内容多次参与 样本计算的问题？

节点本身是在变的 但是也有可能在相当时间内不变 爬虫 回访 和页面重要性

计算 怎么有效解决这些问题 不得不考虑

——————————————————————————————————————————————

垂直搜索 面临的现实问题 （我们可以把专业的站点 看做 是一个 整大的网络 节点 中的一些离散的节点
但这些节点中 肯定有路径相连，在专业内也可以理解为一个小的常规网络)

1. 找到专业内容相关的 节点 爬取(怎么控制在网络内 爬取 爬取专业相关内容)分离不相关节点?

2. 怎么有效计算 大到节点(单个)的排名（在本专业内的排名) 小到搜索结果的页面排名？

3. 爬虫的回访策略 同样怎么有效 避免节点重复计算(无法避免 怎么有效减少这些因素)? 

——————————————————————————————————————————————

在网上看到一篇 《基于效用的定题爬虫搜索策略》 导师：林坤辉    报告人：王　威

我觉得这篇论文非常好 效用来源于经济学重的思想 我觉得这个是重点 判断一个节点 甚至节点内的一

个页面按照有没有价值来决定 呵呵 这个其实和HITS算法的页面算法思想很 相似 基本可以归一

都是判断一个页面的重要程度 HITS是根据 转载和链入来决定重要性 而这篇文章里是 从一个期望效用

出发不断接近真实效用的 我觉得应该叫做一个效用逼近函数 来重复计算 得出页面的重要性 

——————————————————————————————————————————————

所有的这些都只有2个目的 

1. 指导搜索结果排名(搜索引擎 的目的就是让用户能更快的从海量信息中找到 最想要的东西)

2. 找到重要的有效的 节点 指导爬虫爬取 和回访，在垂直搜索中这个 问题 尤为迫切
   怎没保证爬取的内容都是我们需要的 都是专业内容 

——————————————————————————————————————————————

关于怎么确定一个页面是导航页面(列表页面)还是最终页面(文章页)

任老大 那个方法 无可厚非 是一个可行的 方法 而且也是行之有效的方法

我也同意这种做法 但是 系统开发的时候 要考虑系统的复杂度 

能不能找到一种更简单 不一定要比任老大的方法更有效 但是实施上也是完全可行 然后准确率 

也不是低得特离谱的方法呢？

程哥 早上提的那个保底想法 呵呵！ 在他对我提出这个问题时候的 我的第一反应想法 完全一直

起码说明 可行性 极高 但是 既然是保底 那么 准确度上 肯定要想办法提高 

爬虫爬取的时候 如果用任老大的那套方法去实行 肯定是可以的 但是复杂度就大了

本着简单就是美的(python理念)  我还是回到 只知道一个url 怎么来确定他是 文本页还是导航页

的问题上去思考？

对了 保底算法 就是通过判断 本页面内所有指向站内的 url 数目来决定是否是导航页

首先这个首要问题 肯定还是准确率 特别在SEO泛滥的今天 出现了这样一个问题

比文章页面 他的上下左右可能出现很多很多的推荐文章 比喻 今日头条 热点关注 其他用户还看过

之类的文章 这个是导致误判的 很大一个原因 因为他们也是一个小小的列表 而且可能会有很多个列表

确实 这个是符合 用户习惯， 但也是出现 把文章页判断为列表页的 根本原因

通过 查看一些站点 可以总结出这样一个 规律来：
首先 我们把一个站点内的结构看做是层次结构 这个是前提

1. 真正的列表页 在指向本站内所有的链接里 大部分 都是层次关系低于 当前链接的链接

2. 最终文章页面里 指向本站的所有链接里 大部分 都是层次关系等于 当前链接的链接

所以如果每次 我们只知道一个当前链接的 情况下 如果加上面的 这个条件

我们的保底 算法的 准确率提高了 但是复杂度 并不会提高多少 呵呵
——————————————————————————————————————————————

上面判断导航页和列表页 碰到的问题 

尝试实现了上面的模型 对于一般站点 判断准确率很高

但是出现一个问题 就是如果是url rewrite的站点 会产生误判 而且现在url rewrite的站点 越来越多

所以就只有另外想办法了！

我们现有的爬从我可以认为他是一个 广度优先的 爬虫 然后加入 一些过滤机制

在新版爬虫里 我期望的可能是这样的 还是广度优先 加上 过滤机制的层次模型 

然后再加上 内链投票机制 为什么说加上内链投票机制 这会带来多少复杂度？

假设知道一个站点的首页 从首页进去 第一 获取首页上所有链接 写入站点的链接库(假设每个站点我们都

用一个链接库来管理他所有链接) 这些链接的 链入度 就全都投了一票 因为有链接指向它 然后从连接库里

获取没有下载的链接(主页后的第一批) 然后同样每个链接去下载 每个页面提取所有链接入库 如果存在 

就给存在的链接投了一票 这一批获取的下完 然后再去获取所有没下载的(新的一批) 这个时候 我们就可以

根据链入度的投票来排名这些链接下载的优先级别 呵呵 以此来 这样一批一批的就行成了 网站的层次结构 

而且随着链接的越下越多 投票数的差别就越来越大 直到到达层次限定 或者 没有新的链接 

当然下载的时候对限定 所有的链接都在本站点域名下面 并且过滤掉一些没用的 站内链接 

这样这个站点一次下载下来 就对他自己的站内链接做了投票 

为什么要这样做？这样做有效果么？

一个网站 他往往会突出他的重点 或者他的主题 重要的页面 他自己的站点内 指向他的链接就多

这个就是根据 而且那种文章的列表页面 或者某个子频道的首页 网站首页 肯定会有大量的文章页

指向它 这样投票下来 自然就找到了 列表页 呵呵 在第一下载的时候 也许你会说 他是边在做投票

边在下载 这个时侯的投票 还没完全呢  是的 对于未知的 我们只有走一步 认识一点 走一步认识一点

一个积累的过程 呵呵 因为不管他怎么url rewrite他的内容 他站点的重要东西 他肯定会增加多的

链接来突出这些 这样就自然形成了 根据他自己的站内 他一个重要程度的 站内url投票排名

但是下次爬虫对这个网站回访的时候 呵呵 我们回访的时候 就可以优先 对这些

投票出来的重要页面进行回访 避免回访大量 没有更新的文章页 

这个方法存在的问题？

第一次下载一个新的站点 因为所有的页面都要被下载所以不存在什么太大的问题 

但是这样投票 会出下一个这样的问题 往往文章的里面会链接到他的所属的频道列表页面的第一页 

如果他的列表页面存在 很多也 那么那些中间的 列表页面 也许会被投票到的机会不多 呵呵

今天实现了下这个模型 对一个专业站点 进行整站抓取分析 发现 这个办法完全可行 呵呵

——————————————————————————————————————————————

关于新站发现的一点想法

现有的做法
原来我们的做法是 利用现有的搜索引擎 提取结果排名前多少名的站点入库 然后 进行首页文本分类 

按照专业词比例来判断是否是专业网站 确实这样可以发现不少专业网站 但是存在一个问题

就是如果一些专业网站被搜索引擎惩罚了 但是它有确实是专业网站的  这样一类网站会被漏掉

还有一个就是 与行业相关了 如果是冷门行业 获取他们的网站根本不会被百度 Google之类的的收录

今天看到武汉大学的一篇博士论文 

他们是利用 链接数目 外链数目 内链数目 来做学术网站排名的 一篇论文 有点启发

过去我们发现新站 没有充分利用 爬虫已经爬取的数据 过分依赖外来数据

我的想法是 我们可以 对现有的专业爬虫(暂且假设都算质量合格的专业站点)

的所有爬虫的所有页面 作分析  每个页面都找出不属于本host的url 然后提取出这些url的host

入库 如果的时候存在他的投票就加一  这样 我们就可以得到一个庞大的 站外链接的 投票结果库

很显然 google 百度的等那些广告提供商的 页面也许会靠前 但是很容易过滤掉

为什么说这种做法有效？

很多文章说互联网结构是一个有向图  我觉得 应该是无向图更准确 如果是有向图 能说出 为什么有方向

朝那个方向 假设这些 你都解答了 那么还有 为什么要朝你说的方向 所以这是不对的

如果把互联网看成无向图 那么我们的专业站点 就是这些无向图上 一些看似 离散的节点

但是 行业是相关的 因为 如果你是干那一行的 可能你关注的 重点 就在你那一行 

所以我们可以推断这些专业站点 如果做外链 友情链接之类 肯定大部分都是 行业相关站点

基于这一点 可以说 这种方法发现新站点是有效的 

再说怎么过滤掉 外链的投票库里的一些 干扰站点？

很简单 东西有了 挑出 没用的 或者说来确定 价值 还是用老办法 抓取站点 首页

然后进行文本分类 获取首页 专业词 比例 这样很容易把非专业站点过滤掉
```


